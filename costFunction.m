function [J, grad] = costFunction(X, y, theta, lambda)% finds the cost of using a specific theta vector for logistic regression% X is a matrix of observations with size [m, n]% y is a vector of true target values with size [m, 1]% theta is a vector of parameters with size [n, 1]m = size(X,1);% g(z) : theta' * Xgz = X * theta;% h(x) : 1/(1-e^(-gz))hx = sigmoid(gz);% cost if y=1 : y[i]log(h(x[i]))yOneTerm = y .* log(hx);% cost if y=0 : (1-y[i])log(1-h(x[i]))yZeroTerm = (1 - y) .* log(1 - hx);% this is the full cost sumJ = (-1/m) * sum(yOneTerm + yZeroTerm);% add the regularization termJ = J + (lambda/(2*m)) * sum(theta .^ 2);% calculates the gradient for this thetaerror = hx - y;grad = (1/m) * (X' * error);% adds regularization to gradientgradReg = (lambda/m) * theta;gradReg(1) = 0;  % don't regularize the interceptgrad = grad + gradReg;